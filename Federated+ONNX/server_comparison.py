"""
ONNX vs PyTorch ë¹„êµ ì„œë²„
- ì„œë²„ì—ì„œë„ PyTorch vs ONNX ì¶”ë¡  ì‹œê°„ ë¹„êµ
- ëª¨ë¸ í¬ê¸° ë¹„êµ í¬í•¨
- ë¹„êµ ê²°ê³¼ë¥¼ CSV/JSONìœ¼ë¡œ ì €ì¥
"""

import flwr as fl
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.transforms as transforms
import numpy as np
import argparse
from sklearn.metrics import f1_score, classification_report
import medmnist
from medmnist.info import INFO
import csv
import json
from datetime import datetime
import os
import time
import tracemalloc
import gc

# ONNX
import torch.onnx
import onnx
import onnxruntime as ort

# onnxsimì€ ì„ íƒì  ì‚¬ìš© (macOS M1ì—ì„œ segfault ë°œìƒ ê°€ëŠ¥)
try:
    from onnxsim import simplify
    ONNXSIM_AVAILABLE = True
except ImportError:
    ONNXSIM_AVAILABLE = False
    print("âš ï¸ onnxsim ë¯¸ì„¤ì¹˜ - Simplify ê±´ë„ˆëœ€")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ------------------------------------------------------------------------
# SimpleCNN ëª¨ë¸
# ------------------------------------------------------------------------
class SimpleCNN(nn.Module):
    def __init__(self, num_classes=9):
        super(SimpleCNN, self).__init__()
        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)
        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)
        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.dropout = nn.Dropout(0.25)
        self.fc1 = nn.Linear(64 * 3 * 3, 128)
        self.fc2 = nn.Linear(128, num_classes)
        
    def forward(self, x):
        x = self.pool(torch.relu(self.conv1(x)))
        x = self.pool(torch.relu(self.conv2(x)))
        x = self.pool(torch.relu(self.conv3(x)))
        x = x.view(x.size(0), -1)
        x = self.dropout(torch.relu(self.fc1(x)))
        x = self.fc2(x)
        return x

# ------------------------------------------------------------------------
# ì„œë²„ìš© ONNX Manager
# ------------------------------------------------------------------------
class ServerONNXManager:
    def __init__(self, model, save_dir):
        self.pytorch_model = model
        self.save_dir = save_dir
        self.onnx_path = os.path.join(save_dir, 'server_model.onnx')
        self.simplified_path = os.path.join(save_dir, 'server_model_simplified.onnx')
        self.pytorch_path = os.path.join(save_dir, 'server_model.pth')
        self.ort_session = None
        self.input_name = None
        self.output_name = None
        
        # ëª¨ë¸ í¬ê¸°
        self.pytorch_size_kb = 0
        self.onnx_original_size_kb = 0
        self.onnx_simplified_size_kb = 0
    
    def export_and_simplify(self, use_simplifier=False):
        """ONNX ë³€í™˜ ë° Simplify, í¬ê¸° ì¸¡ì •
        
        Args:
            use_simplifier: Simplifier ì‚¬ìš© ì—¬ë¶€ (macOS M1ì—ì„œëŠ” False ê¶Œì¥)
        """
        self.pytorch_model.eval()
        dummy_input = torch.randn(1, 3, 28, 28).to(device)
        
        # PyTorch ëª¨ë¸ í¬ê¸° ì¸¡ì •
        torch.save(self.pytorch_model.state_dict(), self.pytorch_path)
        self.pytorch_size_kb = os.path.getsize(self.pytorch_path) / 1024
        
        # ONNX Export
        torch.onnx.export(
            self.pytorch_model,
            dummy_input,
            self.onnx_path,
            export_params=True,
            opset_version=11,
            do_constant_folding=True,
            input_names=['input'],
            output_names=['output'],
            dynamic_axes={
                'input': {0: 'batch_size'},
                'output': {0: 'batch_size'}
            }
        )
        
        # ONNX ì›ë³¸ í¬ê¸°
        self.onnx_original_size_kb = os.path.getsize(self.onnx_path) / 1024
        
        # Simplify (ì„ íƒì )
        simplified_success = False
        if use_simplifier and ONNXSIM_AVAILABLE:
            try:
                onnx_model = onnx.load(self.onnx_path)
                simplified_model, check = simplify(onnx_model)
                
                if check:
                    onnx.save(simplified_model, self.simplified_path)
                    self.onnx_simplified_size_kb = os.path.getsize(self.simplified_path) / 1024
                    simplified_success = True
                else:
                    self.onnx_simplified_size_kb = self.onnx_original_size_kb
            except Exception as e:
                print(f"  âš ï¸ Simplifier ì˜¤ë¥˜: {e}")
                self.onnx_simplified_size_kb = self.onnx_original_size_kb
        else:
            # Simplifier ë¯¸ì‚¬ìš© ì‹œ ì›ë³¸ í¬ê¸°ë¡œ ì„¤ì •
            self.onnx_simplified_size_kb = self.onnx_original_size_kb
            if not use_simplifier:
                print("  â„¹ï¸ Simplifier ë¯¸ì‚¬ìš© (ì„œë²„)")
        
        return self.simplified_path if simplified_success else self.onnx_path
    
    def get_size_comparison(self):
        """ëª¨ë¸ í¬ê¸° ë¹„êµ ê²°ê³¼ ë°˜í™˜"""
        reduction_from_pytorch = ((self.pytorch_size_kb - self.onnx_simplified_size_kb) 
                                   / self.pytorch_size_kb * 100) if self.pytorch_size_kb > 0 else 0
        reduction_from_original = ((self.onnx_original_size_kb - self.onnx_simplified_size_kb) 
                                    / self.onnx_original_size_kb * 100) if self.onnx_original_size_kb > 0 else 0
        
        return {
            'pytorch_size_kb': self.pytorch_size_kb,
            'onnx_original_size_kb': self.onnx_original_size_kb,
            'onnx_simplified_size_kb': self.onnx_simplified_size_kb,
            'reduction_from_pytorch_pct': reduction_from_pytorch,
            'reduction_from_original_pct': reduction_from_original
        }
    
    def load_session(self, onnx_path):
        self.ort_session = ort.InferenceSession(
            onnx_path,
            providers=['CPUExecutionProvider']
        )
        self.input_name = self.ort_session.get_inputs()[0].name
        self.output_name = self.ort_session.get_outputs()[0].name
    
    def predict(self, data):
        if self.ort_session is None:
            raise RuntimeError("ONNX session not loaded")
        
        if isinstance(data, torch.Tensor):
            data = data.cpu().numpy()
        
        outputs = self.ort_session.run(
            [self.output_name], 
            {self.input_name: data.astype(np.float32)}
        )
        
        return outputs[0]

# ------------------------------------------------------------------------
# ê¸€ë¡œë²Œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
# ------------------------------------------------------------------------
def load_global_test_data(batch_size=32):
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5], std=[0.5])
    ])
    
    info = INFO['pathmnist']
    DataClass = getattr(medmnist, info['python_class'])
    
    test_dataset = DataClass(split='test', transform=transform, download=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)
    
    print(f"âœ… ê¸€ë¡œë²Œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ: {len(test_dataset)}ê°œ")
    return test_loader

# ------------------------------------------------------------------------
# PyTorch ê¸€ë¡œë²Œ í‰ê°€ (tracemalloc í”¼í¬ ë©”ëª¨ë¦¬ ì¸¡ì •)
# ------------------------------------------------------------------------
def evaluate_pytorch(model, test_loader, num_classes=9):
    criterion = nn.CrossEntropyLoss()
    model.eval()
    
    test_loss = 0.0
    correct = 0
    total = 0
    all_predictions = []
    all_targets = []
    
    # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ë° ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
    gc.collect()
    
    # tracemallocìœ¼ë¡œ í”¼í¬ ë©”ëª¨ë¦¬ ì¸¡ì •
    tracemalloc.start()
    tracemalloc.reset_peak()
    
    start_time = time.time()
    
    with torch.no_grad():
        for data, target in test_loader:
            data = data.to(device)
            target = target.to(device).squeeze().long()
            
            output = model(data)
            test_loss += criterion(output, target).item()
            
            _, predicted = torch.max(output, 1)
            correct += (predicted == target).sum().item()
            total += target.size(0)
            
            all_predictions.extend(predicted.cpu().numpy())
            all_targets.extend(target.cpu().numpy())
    
    inference_time = time.time() - start_time
    
    # í”¼í¬ ë©”ëª¨ë¦¬ ì¸¡ì • (MB ë‹¨ìœ„)
    current, peak = tracemalloc.get_traced_memory()
    tracemalloc.stop()
    peak_memory_mb = peak / 1024 / 1024
    
    test_loss /= len(test_loader)
    accuracy = 100. * correct / total
    macro_f1 = f1_score(all_targets, all_predictions, average='macro', zero_division=0)
    per_class_f1 = f1_score(all_targets, all_predictions, average=None, zero_division=0)
    
    per_class_accuracy = []
    for class_idx in range(num_classes):
        class_mask = [t == class_idx for t in all_targets]
        if sum(class_mask) > 0:
            class_correct = sum([1 for p, t, m in zip(all_predictions, all_targets, class_mask) if m and p == t])
            class_acc = 100. * class_correct / sum(class_mask)
            per_class_accuracy.append(class_acc)
        else:
            per_class_accuracy.append(0.0)
    
    return {
        'framework': 'PyTorch',
        'loss': test_loss,
        'accuracy': accuracy,
        'macro_f1': macro_f1,
        'per_class_f1': per_class_f1,
        'per_class_accuracy': per_class_accuracy,
        'inference_time': inference_time,
        'memory_usage': peak_memory_mb
    }

# ------------------------------------------------------------------------
# ONNX ê¸€ë¡œë²Œ í‰ê°€ (tracemalloc í”¼í¬ ë©”ëª¨ë¦¬ ì¸¡ì •)
# ------------------------------------------------------------------------
def evaluate_onnx(onnx_manager, test_loader, num_classes=9):
    all_predictions = []
    all_targets = []
    
    # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ë° ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
    gc.collect()
    
    # tracemallocìœ¼ë¡œ í”¼í¬ ë©”ëª¨ë¦¬ ì¸¡ì •
    tracemalloc.start()
    tracemalloc.reset_peak()
    
    start_time = time.time()
    
    for data, target in test_loader:
        target = target.squeeze().long()
        
        outputs = onnx_manager.predict(data)
        predicted = np.argmax(outputs, axis=1)
        
        all_predictions.extend(predicted)
        all_targets.extend(target.numpy())
    
    inference_time = time.time() - start_time
    
    # í”¼í¬ ë©”ëª¨ë¦¬ ì¸¡ì • (MB ë‹¨ìœ„)
    current, peak = tracemalloc.get_traced_memory()
    tracemalloc.stop()
    peak_memory_mb = peak / 1024 / 1024
    
    correct = sum([1 for p, t in zip(all_predictions, all_targets) if p == t])
    accuracy = 100. * correct / len(all_targets)
    macro_f1 = f1_score(all_targets, all_predictions, average='macro', zero_division=0)
    per_class_f1 = f1_score(all_targets, all_predictions, average=None, zero_division=0)
    
    per_class_accuracy = []
    for class_idx in range(num_classes):
        class_mask = [t == class_idx for t in all_targets]
        if sum(class_mask) > 0:
            class_correct = sum([1 for p, t, m in zip(all_predictions, all_targets, class_mask) if m and p == t])
            class_acc = 100. * class_correct / sum(class_mask)
            per_class_accuracy.append(class_acc)
        else:
            per_class_accuracy.append(0.0)
    
    return {
        'framework': 'ONNX',
        'accuracy': accuracy,
        'macro_f1': macro_f1,
        'per_class_f1': per_class_f1,
        'per_class_accuracy': per_class_accuracy,
        'inference_time': inference_time,
        'memory_usage': peak_memory_mb
    }

# ------------------------------------------------------------------------
# ë¹„êµ ê²°ê³¼ ë¡œê±° (í™•ì¥)
# ------------------------------------------------------------------------
class ComparisonLogger:
    def __init__(self, experiment_name):
        self.experiment_name = experiment_name
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.results_dir = f"comparison_{experiment_name}_{self.timestamp}"
        os.makedirs(self.results_dir, exist_ok=True)
        
        # CSV íŒŒì¼ë“¤
        self.client_csv_file = f"{self.results_dir}/client_comparison_results.csv"
        self.server_csv_file = f"{self.results_dir}/server_comparison_results.csv"
        self.model_size_csv_file = f"{self.results_dir}/model_size_comparison.csv"
        
        self.init_csvs()
        
        # JSON íŒŒì¼
        self.json_file = f"{self.results_dir}/detailed_comparison.json"
        self.results = {
            'client_results': [],
            'server_results': [],
            'model_sizes': []
        }
        
        print(f"ğŸ“ ë¹„êµ ê²°ê³¼ ì €ì¥: {self.results_dir}")
    
    def init_csvs(self):
        # í´ë¼ì´ì–¸íŠ¸ ê²°ê³¼ CSV
        client_headers = ['Round', 'Client_ID', 
                          'PT_Accuracy', 'PT_F1', 'PT_Time', 'PT_Memory',
                          'ONNX_Accuracy', 'ONNX_F1', 'ONNX_Time', 'ONNX_Memory',
                          'Speedup', 'Accuracy_Diff']
        with open(self.client_csv_file, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(client_headers)
        
        # ì„œë²„ ê²°ê³¼ CSV
        server_headers = ['Round', 
                          'PT_Accuracy', 'PT_F1', 'PT_Time', 'PT_Memory',
                          'ONNX_Accuracy', 'ONNX_F1', 'ONNX_Time', 'ONNX_Memory',
                          'Speedup', 'Accuracy_Diff']
        with open(self.server_csv_file, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(server_headers)
        
        # ëª¨ë¸ í¬ê¸° CSV
        size_headers = ['Round', 'Source', 'PyTorch_KB', 'ONNX_Original_KB', 
                        'ONNX_Simplified_KB', 'Reduction_From_PyTorch_Pct', 
                        'Reduction_From_Original_Pct']
        with open(self.model_size_csv_file, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(size_headers)
    
    def log_client_result(self, round_num, metrics):
        """í´ë¼ì´ì–¸íŠ¸ ê²°ê³¼ ê¸°ë¡"""
        row = [
            round_num,
            metrics.get('client_id', 0),
            f"{metrics.get('accuracy', 0):.2f}",
            f"{metrics.get('f1_score', 0):.4f}",
            f"{metrics.get('inference_time', 0):.3f}",
            f"{metrics.get('memory_usage', 0):.2f}",
            f"{metrics.get('onnx_accuracy', 0):.2f}",
            f"{metrics.get('onnx_f1_score', 0):.4f}",
            f"{metrics.get('onnx_inference_time', 0):.3f}",
            f"{metrics.get('onnx_memory_usage', 0):.2f}",
            f"{metrics.get('speedup', 0):.2f}",
            f"{metrics.get('accuracy_diff', 0):.4f}"
        ]
        
        with open(self.client_csv_file, 'a', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(row)
        
        # ëª¨ë¸ í¬ê¸°ë„ ê¸°ë¡
        if 'pytorch_size_kb' in metrics:
            size_row = [
                round_num,
                f"Client_{metrics.get('client_id', 0)}",
                f"{metrics.get('pytorch_size_kb', 0):.2f}",
                f"{metrics.get('onnx_original_size_kb', 0):.2f}",
                f"{metrics.get('onnx_simplified_size_kb', 0):.2f}",
                f"{metrics.get('size_reduction_from_pytorch_pct', 0):.1f}",
                f"{metrics.get('size_reduction_from_original_pct', 0):.1f}"
            ]
            with open(self.model_size_csv_file, 'a', newline='') as f:
                writer = csv.writer(f)
                writer.writerow(size_row)
        
        self.results['client_results'].append({
            'round': round_num,
            **{k: float(v) if isinstance(v, (int, float, np.floating)) else v 
               for k, v in metrics.items()}
        })
        self._save_json()
    
    def log_server_result(self, round_num, pt_results, onnx_results, size_info):
        """ì„œë²„ ê²°ê³¼ ê¸°ë¡"""
        speedup = pt_results['inference_time'] / onnx_results['inference_time'] if onnx_results['inference_time'] > 0 else 0
        acc_diff = abs(pt_results['accuracy'] - onnx_results['accuracy'])
        
        row = [
            round_num,
            f"{pt_results['accuracy']:.2f}",
            f"{pt_results['macro_f1']:.4f}",
            f"{pt_results['inference_time']:.3f}",
            f"{pt_results['memory_usage']:.2f}",
            f"{onnx_results['accuracy']:.2f}",
            f"{onnx_results['macro_f1']:.4f}",
            f"{onnx_results['inference_time']:.3f}",
            f"{onnx_results['memory_usage']:.2f}",
            f"{speedup:.2f}",
            f"{acc_diff:.4f}"
        ]
        
        with open(self.server_csv_file, 'a', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(row)
        
        # ëª¨ë¸ í¬ê¸° ê¸°ë¡
        size_row = [
            round_num,
            "Server",
            f"{size_info['pytorch_size_kb']:.2f}",
            f"{size_info['onnx_original_size_kb']:.2f}",
            f"{size_info['onnx_simplified_size_kb']:.2f}",
            f"{size_info['reduction_from_pytorch_pct']:.1f}",
            f"{size_info['reduction_from_original_pct']:.1f}"
        ]
        with open(self.model_size_csv_file, 'a', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(size_row)
        
        self.results['server_results'].append({
            'round': round_num,
            'pytorch': {
                'accuracy': float(pt_results['accuracy']),
                'macro_f1': float(pt_results['macro_f1']),
                'inference_time': float(pt_results['inference_time']),
                'memory_usage': float(pt_results['memory_usage']),
                'per_class_f1': pt_results['per_class_f1'].tolist(),
                'per_class_accuracy': pt_results['per_class_accuracy']
            },
            'onnx': {
                'accuracy': float(onnx_results['accuracy']),
                'macro_f1': float(onnx_results['macro_f1']),
                'inference_time': float(onnx_results['inference_time']),
                'memory_usage': float(onnx_results['memory_usage']),
                'per_class_f1': onnx_results['per_class_f1'].tolist(),
                'per_class_accuracy': onnx_results['per_class_accuracy']
            },
            'speedup': float(speedup),
            'accuracy_diff': float(acc_diff),
            'model_size': size_info
        })
        
        self.results['model_sizes'].append({
            'round': round_num,
            'source': 'Server',
            **size_info
        })
        
        self._save_json()
    
    def _save_json(self):
        with open(self.json_file, 'w') as f:
            json.dump(self.results, f, indent=2)
    
    def print_server_comparison(self, round_num, pt_results, onnx_results, size_info):
        """ì„œë²„ ë¹„êµ ê²°ê³¼ ì¶œë ¥"""
        speedup = pt_results['inference_time'] / onnx_results['inference_time'] if onnx_results['inference_time'] > 0 else 0
        
        print(f"\n{'='*70}")
        print(f"ë¼ìš´ë“œ {round_num} - ì„œë²„ ê¸€ë¡œë²Œ í‰ê°€ (PyTorch vs ONNX)")
        print(f"{'='*70}")
        
        print(f"\nğŸ“¦ ëª¨ë¸ í¬ê¸°:")
        print(f"  PyTorch (.pth):      {size_info['pytorch_size_kb']:.2f} KB")
        print(f"  ONNX (ì›ë³¸):         {size_info['onnx_original_size_kb']:.2f} KB")
        print(f"  ONNX (Simplified):   {size_info['onnx_simplified_size_kb']:.2f} KB")
        print(f"  PyTorch ëŒ€ë¹„ ì ˆê°:   {size_info['reduction_from_pytorch_pct']:.1f}%")
        print(f"  ONNX ì›ë³¸ ëŒ€ë¹„ ì ˆê°: {size_info['reduction_from_original_pct']:.1f}%")
        
        print(f"\nâš¡ PyTorch:")
        print(f"  Accuracy: {pt_results['accuracy']:.2f}%")
        print(f"  Macro F1: {pt_results['macro_f1']:.4f}")
        print(f"  ì¶”ë¡  ì‹œê°„: {pt_results['inference_time']:.3f}ì´ˆ")
        print(f"  ë©”ëª¨ë¦¬: {pt_results['memory_usage']:.2f}MB")
        
        print(f"\nâš¡ ONNX:")
        print(f"  Accuracy: {onnx_results['accuracy']:.2f}%")
        print(f"  Macro F1: {onnx_results['macro_f1']:.4f}")
        print(f"  ì¶”ë¡  ì‹œê°„: {onnx_results['inference_time']:.3f}ì´ˆ")
        print(f"  ë©”ëª¨ë¦¬: {onnx_results['memory_usage']:.2f}MB")
        
        print(f"\nğŸ“Š ë¹„êµ:")
        print(f"  Speedup: {speedup:.2f}x")
        print(f"  ì •í™•ë„ ì°¨ì´: {abs(pt_results['accuracy'] - onnx_results['accuracy']):.4f}%")
        print(f"{'='*70}\n")
    
    def generate_comparison_summary(self):
        """ìµœì¢… ë¹„êµ ìš”ì•½ ìƒì„±"""
        summary_file = f"{self.results_dir}/comparison_summary.txt"
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write("="*70 + "\n")
            f.write("ONNX vs PyTorch ì¢…í•© ë¹„êµ ê²°ê³¼\n")
            f.write("="*70 + "\n\n")
            
            # ì„œë²„ ê²°ê³¼ ìš”ì•½
            if self.results['server_results']:
                f.write("ğŸ“Š ì„œë²„ ê¸€ë¡œë²Œ í‰ê°€ ê²°ê³¼\n")
                f.write("-"*50 + "\n")
                
                pt_times = [r['pytorch']['inference_time'] for r in self.results['server_results']]
                onnx_times = [r['onnx']['inference_time'] for r in self.results['server_results']]
                speedups = [r['speedup'] for r in self.results['server_results']]
                
                f.write(f"PyTorch í‰ê·  ì¶”ë¡  ì‹œê°„: {np.mean(pt_times):.3f}ì´ˆ\n")
                f.write(f"ONNX í‰ê·  ì¶”ë¡  ì‹œê°„: {np.mean(onnx_times):.3f}ì´ˆ\n")
                f.write(f"í‰ê·  Speedup: {np.mean(speedups):.2f}x\n")
                
                # ìµœì¢… ë¼ìš´ë“œ ê²°ê³¼
                final = self.results['server_results'][-1]
                f.write(f"\nìµœì¢… ë¼ìš´ë“œ ê²°ê³¼:\n")
                f.write(f"  PyTorch Accuracy: {final['pytorch']['accuracy']:.2f}%\n")
                f.write(f"  ONNX Accuracy: {final['onnx']['accuracy']:.2f}%\n")
                f.write(f"  ì •í™•ë„ ì°¨ì´: {final['accuracy_diff']:.4f}%\n\n")
            
            # ëª¨ë¸ í¬ê¸° ìš”ì•½
            if self.results['model_sizes']:
                f.write("ğŸ“¦ ëª¨ë¸ í¬ê¸° ë¹„êµ\n")
                f.write("-"*50 + "\n")
                
                # ì„œë²„ ëª¨ë¸ í¬ê¸° (ë§ˆì§€ë§‰ ë¼ìš´ë“œ)
                server_sizes = [s for s in self.results['model_sizes'] if s['source'] == 'Server']
                if server_sizes:
                    last_size = server_sizes[-1]
                    f.write(f"PyTorch: {last_size['pytorch_size_kb']:.2f} KB\n")
                    f.write(f"ONNX (ì›ë³¸): {last_size['onnx_original_size_kb']:.2f} KB\n")
                    f.write(f"ONNX (Simplified): {last_size['onnx_simplified_size_kb']:.2f} KB\n")
                    f.write(f"PyTorch ëŒ€ë¹„ ì ˆê°: {last_size['reduction_from_pytorch_pct']:.1f}%\n")
                    f.write(f"Simplify ì ˆê°: {last_size['reduction_from_original_pct']:.1f}%\n\n")
            
            # í´ë¼ì´ì–¸íŠ¸ ê²°ê³¼ ìš”ì•½
            if self.results['client_results']:
                f.write("ğŸ“± í´ë¼ì´ì–¸íŠ¸ë³„ ê²°ê³¼\n")
                f.write("-"*50 + "\n")
                
                client_ids = set([r.get('client_id', 0) for r in self.results['client_results']])
                
                for client_id in sorted(client_ids):
                    client_data = [r for r in self.results['client_results'] if r.get('client_id') == client_id]
                    
                    if client_data:
                        avg_speedup = np.mean([r.get('speedup', 0) for r in client_data])
                        avg_acc_diff = np.mean([r.get('accuracy_diff', 0) for r in client_data])
                        
                        f.write(f"\nClient {client_id}:\n")
                        f.write(f"  í‰ê·  Speedup: {avg_speedup:.2f}x\n")
                        f.write(f"  í‰ê·  ì •í™•ë„ ì°¨ì´: {avg_acc_diff:.4f}%\n")
            
            f.write("\n" + "="*70 + "\n")
        
        print(f"ğŸ“Š ë¹„êµ ìš”ì•½ ì €ì¥: {summary_file}")

# ------------------------------------------------------------------------
# Custom Strategy (ì„œë²„ ONNX í‰ê°€ ì¶”ê°€)
# ------------------------------------------------------------------------
class ComparisonStrategy(fl.server.strategy.FedAvg):
    def __init__(self, model, test_loader, logger, num_classes=9, **kwargs):
        super().__init__(**kwargs)
        self.model = model
        self.test_loader = test_loader
        self.logger = logger
        self.num_classes = num_classes
        self.current_round = 0
        self.onnx_manager = ServerONNXManager(model, logger.results_dir)
    
    def aggregate_fit(self, server_round, results, failures):
        self.current_round = server_round
        
        aggregated_parameters, aggregated_metrics = super().aggregate_fit(
            server_round, results, failures
        )
        
        if aggregated_parameters is not None:
            aggregated_ndarrays = fl.common.parameters_to_ndarrays(aggregated_parameters)
            params_dict = zip(self.model.state_dict().keys(), aggregated_ndarrays)
            state_dict = {k: torch.tensor(v) for k, v in params_dict}
            self.model.load_state_dict(state_dict, strict=True)
            
            print(f"\nğŸ” ë¼ìš´ë“œ {server_round} ì„œë²„ ê¸€ë¡œë²Œ í‰ê°€ ì¤‘...")
            
            # 1. PyTorch í‰ê°€
            print("âš¡ PyTorch ì¶”ë¡  ì¤‘...")
            pt_results = evaluate_pytorch(self.model, self.test_loader, self.num_classes)
            
            # 2. ONNX ë³€í™˜ ë° í‰ê°€
            print("ğŸ”„ ONNX ë³€í™˜ ì¤‘...")
            onnx_path = self.onnx_manager.export_and_simplify(use_simplifier=False)  # macOS M1 í˜¸í™˜
            self.onnx_manager.load_session(onnx_path)
            size_info = self.onnx_manager.get_size_comparison()
            
            print("âš¡ ONNX ì¶”ë¡  ì¤‘...")
            onnx_results = evaluate_onnx(self.onnx_manager, self.test_loader, self.num_classes)
            
            # ê²°ê³¼ ê¸°ë¡
            self.logger.log_server_result(server_round, pt_results, onnx_results, size_info)
            self.logger.print_server_comparison(server_round, pt_results, onnx_results, size_info)
        
        return aggregated_parameters, aggregated_metrics
    
    def aggregate_evaluate(self, server_round, results, failures):
        """í´ë¼ì´ì–¸íŠ¸ í‰ê°€ ê²°ê³¼ ìˆ˜ì§‘ ë° ê¸°ë¡"""
        if not results:
            return None, {}
        
        # ê° í´ë¼ì´ì–¸íŠ¸ ê²°ê³¼ ê¸°ë¡
        for client_proxy, evaluate_res in results:
            metrics = evaluate_res.metrics
            if metrics and 'client_id' in metrics:
                self.logger.log_client_result(server_round, metrics)
        
        return super().aggregate_evaluate(server_round, results, failures)

# ------------------------------------------------------------------------
# ë©”ì¸
# ------------------------------------------------------------------------
def main():
    parser = argparse.ArgumentParser(description='ONNX vs PyTorch ë¹„êµ ì„œë²„')
    parser.add_argument('--server_address', type=str, default='192.168.45.100:8080')
    parser.add_argument('--num_rounds', type=int, default=20)
    parser.add_argument('--min_clients', type=int, default=3)
    parser.add_argument('--experiment_name', type=str, default='onnx_comparison')
    parser.add_argument('--batch_size', type=int, default=16)
    
    args = parser.parse_args()
    
    print("\n" + "="*70)
    print("ONNX vs PyTorch ë¹„êµ ì„œë²„ (ì„œë²„ ì¸¡ ONNX í‰ê°€ í¬í•¨)")
    print("="*70)
    print(f"  ì‹¤í—˜ ì´ë¦„: {args.experiment_name}")
    print(f"  ë¼ìš´ë“œ: {args.num_rounds}")
    print("="*70 + "\n")
    
    test_loader = load_global_test_data(args.batch_size)
    model = SimpleCNN(num_classes=9).to(device)
    logger = ComparisonLogger(args.experiment_name)
    
    initial_parameters = [val.cpu().numpy() for val in model.state_dict().values()]
    
    strategy = ComparisonStrategy(
        model=model,
        test_loader=test_loader,
        logger=logger,
        num_classes=9,
        fraction_fit=1.0,
        fraction_evaluate=1.0,
        min_fit_clients=args.min_clients,
        min_evaluate_clients=args.min_clients,
        min_available_clients=args.min_clients,
        initial_parameters=fl.common.ndarrays_to_parameters(initial_parameters),
        evaluate_metrics_aggregation_fn=None,
    )
    
    print(f"ğŸš€ ì„œë²„ ì‹œì‘: {args.server_address}\n")
    
    fl.server.start_server(
        server_address=args.server_address,
        config=fl.server.ServerConfig(num_rounds=args.num_rounds),
        strategy=strategy
    )
    
    logger.generate_comparison_summary()
    
    print("\n" + "="*70)
    print("âœ… ë¹„êµ ì‹¤í—˜ ì™„ë£Œ!")
    print(f"ğŸ“ ê²°ê³¼: {logger.results_dir}")
    print("="*70 + "\n")

if __name__ == "__main__":
    main()