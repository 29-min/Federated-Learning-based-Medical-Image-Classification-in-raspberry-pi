"""
ONNX vs PyTorch ë¹„êµ í´ë¼ì´ì–¸íŠ¸
- ê° ë¼ìš´ë“œë§ˆë‹¤ PyTorchì™€ ONNX ì–‘ìª½ìœ¼ë¡œ ì¶”ë¡ 
- ì¶”ë¡  ì‹œê°„, ì •í™•ë„, F1, ëª¨ë¸ í¬ê¸° ëª¨ë‘ ì¸¡ì •í•˜ì—¬ ì„œë²„ë¡œ ì „ì†¡
"""

import flwr as fl
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np
import argparse
from sklearn.metrics import f1_score
import os
import time
import tracemalloc
import gc

# ONNX
import torch.onnx
import onnx
from onnxsim import simplify
import onnxruntime as ort

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ------------------------------------------------------------------------
# SimpleCNN ëª¨ë¸
# ------------------------------------------------------------------------
class SimpleCNN(nn.Module):
    def __init__(self, num_classes=9):
        super(SimpleCNN, self).__init__()
        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)
        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)
        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.dropout = nn.Dropout(0.25)
        self.fc1 = nn.Linear(64 * 3 * 3, 128)
        self.fc2 = nn.Linear(128, num_classes)
        
    def forward(self, x):
        x = self.pool(torch.relu(self.conv1(x)))
        x = self.pool(torch.relu(self.conv2(x)))
        x = self.pool(torch.relu(self.conv3(x)))
        x = x.view(x.size(0), -1)
        x = self.dropout(torch.relu(self.fc1(x)))
        x = self.fc2(x)
        return x

# ------------------------------------------------------------------------
# ONNX Manager (ëª¨ë¸ í¬ê¸° ì¸¡ì • ì¶”ê°€)
# ------------------------------------------------------------------------
class ONNXManager:
    def __init__(self, model, client_id):
        self.pytorch_model = model
        self.client_id = client_id
        self.onnx_path = f'model_client_{client_id}.onnx'
        self.simplified_path = f'model_client_{client_id}_simplified.onnx'
        self.pytorch_path = f'model_client_{client_id}.pth'
        self.ort_session = None
        self.input_name = None
        self.output_name = None
        
        # ëª¨ë¸ í¬ê¸° ì €ì¥
        self.pytorch_size_kb = 0
        self.onnx_original_size_kb = 0
        self.onnx_simplified_size_kb = 0
    
    def get_pytorch_size(self):
        """PyTorch ëª¨ë¸ í¬ê¸° ì¸¡ì •"""
        torch.save(self.pytorch_model.state_dict(), self.pytorch_path)
        size_bytes = os.path.getsize(self.pytorch_path)
        self.pytorch_size_kb = size_bytes / 1024
        return self.pytorch_size_kb
    
    def export_and_simplify(self):
        """ONNX ë³€í™˜ ë° Simplify, í¬ê¸° ì¸¡ì •"""
        self.pytorch_model.eval()
        dummy_input = torch.randn(1, 3, 28, 28).to(device)
        
        # PyTorch ëª¨ë¸ í¬ê¸° ì¸¡ì •
        self.get_pytorch_size()
        
        # ONNX Export
        torch.onnx.export(
            self.pytorch_model,
            dummy_input,
            self.onnx_path,
            export_params=True,
            opset_version=11,
            do_constant_folding=True,
            input_names=['input'],
            output_names=['output'],
            dynamic_axes={
                'input': {0: 'batch_size'},
                'output': {0: 'batch_size'}
            }
        )
        
        # ONNX ì›ë³¸ í¬ê¸°
        self.onnx_original_size_kb = os.path.getsize(self.onnx_path) / 1024
        
        # Simplify
        simplified_success = False
        try:
            onnx_model = onnx.load(self.onnx_path)
            simplified_model, check = simplify(onnx_model)
            
            if check:
                onnx.save(simplified_model, self.simplified_path)
                self.onnx_simplified_size_kb = os.path.getsize(self.simplified_path) / 1024
                simplified_success = True
            else:
                self.onnx_simplified_size_kb = self.onnx_original_size_kb
        except Exception as e:
            print(f"  âš ï¸ Simplifier ì˜¤ë¥˜: {e}")
            self.onnx_simplified_size_kb = self.onnx_original_size_kb
        
        return self.simplified_path if simplified_success else self.onnx_path
    
    def get_size_comparison(self):
        """ëª¨ë¸ í¬ê¸° ë¹„êµ ê²°ê³¼ ë°˜í™˜"""
        reduction_from_pytorch = ((self.pytorch_size_kb - self.onnx_simplified_size_kb) 
                                   / self.pytorch_size_kb * 100) if self.pytorch_size_kb > 0 else 0
        reduction_from_original = ((self.onnx_original_size_kb - self.onnx_simplified_size_kb) 
                                    / self.onnx_original_size_kb * 100) if self.onnx_original_size_kb > 0 else 0
        
        return {
            'pytorch_size_kb': self.pytorch_size_kb,
            'onnx_original_size_kb': self.onnx_original_size_kb,
            'onnx_simplified_size_kb': self.onnx_simplified_size_kb,
            'reduction_from_pytorch_pct': reduction_from_pytorch,
            'reduction_from_original_pct': reduction_from_original
        }
    
    def load_session(self, onnx_path):
        self.ort_session = ort.InferenceSession(
            onnx_path,
            providers=['CPUExecutionProvider']
        )
        self.input_name = self.ort_session.get_inputs()[0].name
        self.output_name = self.ort_session.get_outputs()[0].name
    
    def predict(self, data):
        if self.ort_session is None:
            raise RuntimeError("ONNX session not loaded")
        
        if isinstance(data, torch.Tensor):
            data = data.cpu().numpy()
        
        outputs = self.ort_session.run(
            [self.output_name], 
            {self.input_name: data.astype(np.float32)}
        )
        
        return outputs[0]
    
    def cleanup(self):
        """ì„ì‹œ íŒŒì¼ ì •ë¦¬"""
        for path in [self.pytorch_path, self.onnx_path, self.simplified_path]:
            if os.path.exists(path):
                try:
                    os.remove(path)
                except:
                    pass

# ------------------------------------------------------------------------
# ë°ì´í„° ë¡œë“œ
# ------------------------------------------------------------------------
def load_preprocessed_data(client_id, batch_size):
    file_path = f'client_{client_id}_data.pt'
    
    if not os.path.exists(file_path):
        print(f"âŒ ì˜¤ë¥˜: {file_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        exit(1)
        
    print(f"ğŸ“‚ íŒŒì¼ ë¡œë“œ: {file_path}")
    
    data = torch.load(file_path, map_location=device, weights_only=False)
    train_subset = data['train']
    val_subset = data['val']
    num_classes = data['num_classes']

    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=0)
    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, num_workers=0)
    
    print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: Train {len(train_subset)}, Val {len(val_subset)}")
    
    return train_loader, val_loader, num_classes

# ------------------------------------------------------------------------
# í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜
# ------------------------------------------------------------------------
def calculate_class_weights(train_loader, num_classes=9):
    class_counts = torch.zeros(num_classes)
    
    if len(train_loader.dataset) == 0:
        return torch.ones(num_classes)
    
    for _, labels in train_loader:
        labels = labels.squeeze()
        for label in labels:
            class_counts[label.item()] += 1
    
    total = class_counts.sum()
    class_weights = total / (num_classes * class_counts)
    class_weights[class_counts == 0] = 0.0
    
    return class_weights

# ------------------------------------------------------------------------
# í•™ìŠµ
# ------------------------------------------------------------------------
def train(model, train_loader, epochs, lr):
    class_weights = calculate_class_weights(train_loader).to(device)
    criterion = nn.CrossEntropyLoss(weight=class_weights)
    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)
    
    model.train()
    
    for epoch in range(epochs):
        running_loss = 0.0
        correct = 0
        total = 0
        
        for data, target in train_loader:
            data = data.to(device)
            target = target.to(device).squeeze().long()
            
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()
            
            running_loss += loss.item()
            _, predicted = torch.max(output.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()
        
        epoch_loss = running_loss / len(train_loader)
        epoch_acc = 100. * correct / total
        
        print(f"  Epoch {epoch+1}/{epochs}: Loss={epoch_loss:.4f}, Acc={epoch_acc:.2f}%")
    
    return running_loss / len(train_loader)

# ------------------------------------------------------------------------
# PyTorch ì¶”ë¡  (tracemalloc í”¼í¬ ë©”ëª¨ë¦¬ ì¸¡ì •)
# ------------------------------------------------------------------------
def test_pytorch(model, test_loader):
    model.eval()
    
    all_predictions = []
    all_targets = []
    
    # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ë° ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
    gc.collect()
    
    # tracemallocìœ¼ë¡œ í”¼í¬ ë©”ëª¨ë¦¬ ì¸¡ì •
    tracemalloc.start()
    tracemalloc.reset_peak()
    
    start_time = time.time()
    
    with torch.no_grad():
        for data, target in test_loader:
            data = data.to(device)
            target = target.squeeze().long()
            
            output = model(data)
            _, predicted = torch.max(output, 1)
            
            all_predictions.extend(predicted.cpu().numpy())
            all_targets.extend(target.numpy())
    
    total_time = time.time() - start_time
    
    # í”¼í¬ ë©”ëª¨ë¦¬ ì¸¡ì • (MB ë‹¨ìœ„)
    current, peak = tracemalloc.get_traced_memory()
    tracemalloc.stop()
    peak_memory_mb = peak / 1024 / 1024
    
    correct = sum([1 for p, t in zip(all_predictions, all_targets) if p == t])
    accuracy = 100. * correct / len(all_targets)
    f1 = f1_score(all_targets, all_predictions, average='macro', zero_division=0)
    
    return accuracy, f1, total_time, peak_memory_mb

# ------------------------------------------------------------------------
# ONNX ì¶”ë¡  (tracemalloc í”¼í¬ ë©”ëª¨ë¦¬ ì¸¡ì •)
# ------------------------------------------------------------------------
def test_onnx(onnx_manager, test_loader):
    all_predictions = []
    all_targets = []
    
    # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ë° ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
    gc.collect()
    
    # tracemallocìœ¼ë¡œ í”¼í¬ ë©”ëª¨ë¦¬ ì¸¡ì •
    tracemalloc.start()
    tracemalloc.reset_peak()
    
    start_time = time.time()
    
    for data, target in test_loader:
        target = target.squeeze().long()
        
        outputs = onnx_manager.predict(data)
        predicted = np.argmax(outputs, axis=1)
        
        all_predictions.extend(predicted)
        all_targets.extend(target.numpy())
    
    total_time = time.time() - start_time
    
    # í”¼í¬ ë©”ëª¨ë¦¬ ì¸¡ì • (MB ë‹¨ìœ„)
    current, peak = tracemalloc.get_traced_memory()
    tracemalloc.stop()
    peak_memory_mb = peak / 1024 / 1024
    
    correct = sum([1 for p, t in zip(all_predictions, all_targets) if p == t])
    accuracy = 100. * correct / len(all_targets)
    f1 = f1_score(all_targets, all_predictions, average='macro', zero_division=0)
    
    return accuracy, f1, total_time, peak_memory_mb

# ------------------------------------------------------------------------
# Flower Client
# ------------------------------------------------------------------------
class ComparisonClient(fl.client.NumPyClient):
    def __init__(self, client_id, model, train_loader, val_loader, local_epochs, lr):
        self.client_id = client_id
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.local_epochs = local_epochs
        self.lr = lr
        self.current_round = 0
        self.onnx_manager = ONNXManager(model, client_id)

    def get_parameters(self, config):
        return [val.cpu().numpy() for _, val in self.model.state_dict().items()]

    def set_parameters(self, parameters):
        params_dict = zip(self.model.state_dict().keys(), parameters)
        state_dict = {k: torch.tensor(v) for k, v in params_dict}
        self.model.load_state_dict(state_dict, strict=True)

    def fit(self, parameters, config):
        self.current_round += 1
        
        print(f"\n{'='*60}")
        print(f"í´ë¼ì´ì–¸íŠ¸ {self.client_id} - ë¼ìš´ë“œ {self.current_round}")
        print(f"{'='*60}")
        
        self.set_parameters(parameters)
        
        if len(self.train_loader.dataset) == 0:
            return self.get_parameters(config={}), 0, {}
        
        print("ğŸ”¥ PyTorch í•™ìŠµ ì¤‘...")
        train_loss = train(self.model, self.train_loader, self.local_epochs, self.lr)
        
        print(f"âœ… í•™ìŠµ ì™„ë£Œ: Loss={train_loss:.4f}")
        print(f"{'='*60}\n")
        
        return self.get_parameters(config={}), len(self.train_loader.dataset), {}

    def evaluate(self, parameters, config):
        print(f"\n{'='*60}")
        print(f"í´ë¼ì´ì–¸íŠ¸ {self.client_id} - ë¼ìš´ë“œ {self.current_round} ë¹„êµ í‰ê°€")
        print(f"{'='*60}")
        
        self.set_parameters(parameters)
        
        if len(self.val_loader.dataset) == 0:
            return 0.0, 0, {}
        
        # 1. PyTorch ì¶”ë¡ 
        print("âš¡ PyTorch ì¶”ë¡  ì¤‘...")
        pt_acc, pt_f1, pt_time, pt_mem = test_pytorch(self.model, self.val_loader)
        print(f"  PyTorch - Acc: {pt_acc:.2f}%, F1: {pt_f1:.4f}, Time: {pt_time:.3f}ì´ˆ, Mem: {pt_mem:.2f}MB")
        
        # 2. ONNX ë³€í™˜ ë° ì¶”ë¡ 
        print("ğŸ”„ ONNX ë³€í™˜ ì¤‘...")
        onnx_path = self.onnx_manager.export_and_simplify()
        self.onnx_manager.load_session(onnx_path)
        
        # ëª¨ë¸ í¬ê¸° ì¶œë ¥
        size_info = self.onnx_manager.get_size_comparison()
        print(f"\nğŸ“¦ ëª¨ë¸ í¬ê¸° ë¹„êµ:")
        print(f"  PyTorch (.pth):      {size_info['pytorch_size_kb']:.2f} KB")
        print(f"  ONNX (ì›ë³¸):         {size_info['onnx_original_size_kb']:.2f} KB")
        print(f"  ONNX (Simplified):   {size_info['onnx_simplified_size_kb']:.2f} KB")
        print(f"  PyTorch ëŒ€ë¹„ ì ˆê°:   {size_info['reduction_from_pytorch_pct']:.1f}%")
        print(f"  ONNX ì›ë³¸ ëŒ€ë¹„ ì ˆê°: {size_info['reduction_from_original_pct']:.1f}%")
        
        print("\nâš¡ ONNX ì¶”ë¡  ì¤‘...")
        onnx_acc, onnx_f1, onnx_time, onnx_mem = test_onnx(self.onnx_manager, self.val_loader)
        print(f"  ONNX - Acc: {onnx_acc:.2f}%, F1: {onnx_f1:.4f}, Time: {onnx_time:.3f}ì´ˆ, Mem: {onnx_mem:.2f}MB")
        
        # ë¹„êµ
        speedup = pt_time / onnx_time if onnx_time > 0 else 0
        acc_diff = abs(pt_acc - onnx_acc)
        mem_saving = pt_mem - onnx_mem
        mem_saving_pct = (mem_saving / pt_mem * 100) if pt_mem > 0 else 0
        
        print(f"\nğŸ“Š ì„±ëŠ¥ ë¹„êµ:")
        print(f"  Speedup: {speedup:.2f}x")
        print(f"  ì •í™•ë„ ì°¨ì´: {acc_diff:.4f}%")
        print(f"  ë©”ëª¨ë¦¬ ì ˆê°: {mem_saving:.2f}MB ({mem_saving_pct:.1f}%)")
        print(f"{'='*60}\n")
        
        # ê²°ê³¼ ë°˜í™˜ (ëª¨ë¸ í¬ê¸° ì •ë³´ í¬í•¨)
        return 0.0, len(self.val_loader.dataset), {
            'client_id': self.client_id,
            'framework': 'PyTorch',
            'accuracy': pt_acc,
            'f1_score': pt_f1,
            'inference_time': pt_time,
            'memory_usage': pt_mem,
            'onnx_accuracy': onnx_acc,
            'onnx_f1_score': onnx_f1,
            'onnx_inference_time': onnx_time,
            'onnx_memory_usage': onnx_mem,
            'speedup': speedup,
            'accuracy_diff': acc_diff,
            'memory_saving': mem_saving,
            'memory_saving_pct': mem_saving_pct,
            # ëª¨ë¸ í¬ê¸° ì •ë³´ ì¶”ê°€
            'pytorch_size_kb': size_info['pytorch_size_kb'],
            'onnx_original_size_kb': size_info['onnx_original_size_kb'],
            'onnx_simplified_size_kb': size_info['onnx_simplified_size_kb'],
            'size_reduction_from_pytorch_pct': size_info['reduction_from_pytorch_pct'],
            'size_reduction_from_original_pct': size_info['reduction_from_original_pct']
        }

# ------------------------------------------------------------------------
# ë©”ì¸
# ------------------------------------------------------------------------
def main():
    parser = argparse.ArgumentParser(description='ONNX vs PyTorch ë¹„êµ í´ë¼ì´ì–¸íŠ¸')
    parser.add_argument('--client_id', type=int, required=True)
    parser.add_argument('--server_address', type=str, default='192.168.45.100:8080')
    parser.add_argument('--local_epochs', type=int, default=5)
    parser.add_argument('--batch_size', type=int, default=16)
    parser.add_argument('--lr', type=float, default=0.001)
    
    args = parser.parse_args()
    
    print("\n" + "="*60)
    print(f"ONNX vs PyTorch ë¹„êµ í´ë¼ì´ì–¸íŠ¸ {args.client_id}")
    print("="*60)
    
    torch.manual_seed(42)
    np.random.seed(42)
    
    train_loader, val_loader, num_classes = load_preprocessed_data(
        args.client_id, args.batch_size
    )
    
    model = SimpleCNN(num_classes=num_classes).to(device)
    
    client = ComparisonClient(
        args.client_id, model, train_loader, val_loader,
        args.local_epochs, args.lr
    )
    
    print(f"\nğŸ”— ì„œë²„ {args.server_address}ì— ì—°ê²° ì¤‘...\n")
    
    fl.client.start_client(
        server_address=args.server_address,
        client=client
    )
    
    print(f"\ní´ë¼ì´ì–¸íŠ¸ {args.client_id} ì¢…ë£Œ")

if __name__ == "__main__":
    main()