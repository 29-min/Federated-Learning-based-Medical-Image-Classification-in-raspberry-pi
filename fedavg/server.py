"""
ê°œì„ ëœ ì—°í•©í•™ìŠµ ì„œë²„ - ê¸€ë¡œë²Œ í‰ê°€ ê¸°ëŠ¥ ì¶”ê°€
ëª©í‘œ: PathMNIST ì „ì²´ í…ŒìŠ¤íŠ¸ì…‹ìœ¼ë¡œ ê¸€ë¡œë²Œ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
"""

import flwr as fl
from flwr.server.strategy import FedAvg
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.transforms as transforms
from typing import Dict, List, Tuple, Optional
import argparse
import os
import medmnist
from medmnist.info import INFO
from sklearn.metrics import f1_score, classification_report, confusion_matrix
import numpy as np
import pandas as pd
import json
from datetime import datetime


class SimpleCNN(nn.Module):
    """SimpleCNN - PathMNISTìš© (9ê°œ í´ë˜ìŠ¤)"""
    def __init__(self, num_classes=9):
        super(SimpleCNN, self).__init__()
        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)
        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)
        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.dropout = nn.Dropout(0.25)
        self.fc1 = nn.Linear(64 * 3 * 3, 128)
        self.fc2 = nn.Linear(128, num_classes)
        
    def forward(self, x):
        x = self.pool(torch.relu(self.conv1(x)))
        x = self.pool(torch.relu(self.conv2(x)))
        x = self.pool(torch.relu(self.conv3(x)))
        x = x.view(x.size(0), -1)
        x = self.dropout(torch.relu(self.fc1(x)))
        x = self.fc2(x)
        return x


def load_global_test_data(batch_size=128):
    """
    ê¸€ë¡œë²Œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¡œë“œ
    - ëª¨ë“  í´ë˜ìŠ¤ê°€ í¬í•¨ëœ ì „ì²´ í…ŒìŠ¤íŠ¸ì…‹
    - í´ë¼ì´ì–¸íŠ¸ì—ê²Œ ë¶„í• ë˜ì§€ ì•ŠìŒ
    """
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5], std=[0.5])
    ])
    
    data_flag = 'pathmnist'
    info = INFO[data_flag]
    DataClass = getattr(medmnist, info['python_class'])
    
    # ì „ì²´ í…ŒìŠ¤íŠ¸ ë°ì´í„° (ë¶„í• í•˜ì§€ ì•ŠìŒ!)
    test_dataset = DataClass(split='test', transform=transform, download=True)
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=0
    )
    
    print(f"\n{'='*70}")
    print(f"ğŸŒ ê¸€ë¡œë²Œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ")
    print(f"{'='*70}")
    print(f"  - ì´ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜: {len(test_dataset)}")
    print(f"  - ë°°ì¹˜ í¬ê¸°: {batch_size}")
    print(f"  - í´ë˜ìŠ¤ ìˆ˜: {len(info['label'])}")
    print(f"  âš ï¸  ì´ ë°ì´í„°ëŠ” í´ë¼ì´ì–¸íŠ¸ë“¤ì—ê²Œ ë¶„í• ë˜ì§€ ì•Šì€ ì „ì²´ í…ŒìŠ¤íŠ¸ì…‹ì…ë‹ˆë‹¤.")
    print(f"  âœ… ëª¨ë¸ì˜ ì§„ì§œ ì¼ë°˜í™” ì„±ëŠ¥ì„ ì¸¡ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")
    print(f"{'='*70}\n")
    
    return test_loader, len(info['label'])


def get_parameters(model):
    """ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
    return [val.cpu().numpy() for _, val in model.state_dict().items()]


def set_parameters(model, parameters):
    """íŒŒë¼ë¯¸í„°ë¥¼ ëª¨ë¸ì— ì ìš©"""
    params_dict = zip(model.state_dict().keys(), parameters)
    state_dict = {k: torch.tensor(v) for k, v in params_dict}
    model.load_state_dict(state_dict, strict=True)


def evaluate_global_model(model, test_loader, device, round_num, num_classes=9):
    """
    ê¸€ë¡œë²Œ ëª¨ë¸ì„ ì „ì²´ í…ŒìŠ¤íŠ¸ì…‹ìœ¼ë¡œ í‰ê°€
    - ì „ì²´ ì •í™•ë„, F1 score
    - í´ë˜ìŠ¤ë³„ ì •í™•ë„
    - Confusion matrix
    """
    model.to(device)
    model.eval()
    
    criterion = nn.CrossEntropyLoss()
    test_loss = 0.0
    correct = 0
    total = 0
    
    all_predictions = []
    all_targets = []
    all_probs = []
    
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device).squeeze().long()
            output = model(data)
            
            test_loss += criterion(output, target).item()
            
            # Softmax probabilities
            probs = torch.softmax(output, dim=1)
            all_probs.extend(probs.cpu().numpy())
            
            _, predicted = torch.max(output.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()
            
            all_predictions.extend(predicted.cpu().numpy())
            all_targets.extend(target.cpu().numpy())
    
    # ë©”íŠ¸ë¦­ ê³„ì‚°
    test_loss = test_loss / len(test_loader)
    accuracy = 100. * correct / total
    f1_macro = f1_score(all_targets, all_predictions, average='macro', zero_division=0)
    f1_weighted = f1_score(all_targets, all_predictions, average='weighted', zero_division=0)
    
    # í´ë˜ìŠ¤ë³„ ì •í™•ë„
    conf_matrix = confusion_matrix(all_targets, all_predictions, labels=list(range(num_classes)))
    class_accuracy = conf_matrix.diagonal() / conf_matrix.sum(axis=1)
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"\n{'='*70}")
    print(f"ğŸŒ ROUND {round_num} - GLOBAL MODEL EVALUATION (Unbiased Test Set)")
    print(f"{'='*70}")
    print(f"  ğŸ“Š Overall Performance:")
    print(f"     - Test Loss:           {test_loss:.4f}")
    print(f"     - Test Accuracy:       {accuracy:.2f}%")
    print(f"     - F1 Score (Macro):    {f1_macro:.4f}")
    print(f"     - F1 Score (Weighted): {f1_weighted:.4f}")
    print(f"\n  ğŸ“ˆ Per-Class Accuracy:")
    for i in range(num_classes):
        bar_length = int(class_accuracy[i] * 30)
        bar = 'â–ˆ' * bar_length + 'â–‘' * (30 - bar_length)
        print(f"     Class {i}: {class_accuracy[i]*100:>5.1f}%  {bar}")
    print(f"{'='*70}\n")
    
    results = {
        'round': round_num,
        'loss': test_loss,
        'accuracy': accuracy,
        'f1_macro': f1_macro,
        'f1_weighted': f1_weighted,
        'class_accuracy': class_accuracy.tolist(),
        'confusion_matrix': conf_matrix.tolist()
    }
    
    return results


def save_server_checkpoint(model, round_num, checkpoint_dir="checkpoints/server"):
    """ì„œë²„ ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
    os.makedirs(checkpoint_dir, exist_ok=True)
    checkpoint_path = f"{checkpoint_dir}/round_{round_num}.pt"
    torch.save({
        'round': round_num,
        'model_state_dict': model.state_dict(),
    }, checkpoint_path)
    print(f"ğŸ’¾ ì„œë²„ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {checkpoint_path}")


def load_server_checkpoint(model, checkpoint_dir="checkpoints/server"):
    """ì„œë²„ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
    import glob
    
    checkpoints = glob.glob(f"{checkpoint_dir}/round_*.pt")
    
    if checkpoints:
        latest_checkpoint = max(checkpoints, key=lambda x: int(x.split('_')[-1].split('.')[0]))
        checkpoint = torch.load(latest_checkpoint)
        model.load_state_dict(checkpoint['model_state_dict'])
        round_num = checkpoint['round']
        
        print(f"âœ… ì„œë²„ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {latest_checkpoint}")
        print(f"   ë¼ìš´ë“œ {round_num}ë¶€í„° ì¬ê°œí•©ë‹ˆë‹¤.")
        return round_num
    else:
        print(f"ğŸ†• ìƒˆë¡œìš´ ì„œë²„ í•™ìŠµ ì‹œì‘")
        return 0


class GlobalEvaluationStrategy(FedAvg):
    """
    ê¸€ë¡œë²Œ í‰ê°€ ê¸°ëŠ¥ì´ ì¶”ê°€ëœ FedAvg ì „ëµ
    - ë§¤ ë¼ìš´ë“œë§ˆë‹¤ ê¸€ë¡œë²Œ í…ŒìŠ¤íŠ¸ì…‹ìœ¼ë¡œ í‰ê°€
    - ê²°ê³¼ë¥¼ CSV/JSONìœ¼ë¡œ ì €ì¥
    """
    
    def __init__(
        self, 
        model, 
        global_test_loader,
        num_classes=9,
        checkpoint_dir="checkpoints/server",
        results_dir="results",
        **kwargs
    ):
        super().__init__(**kwargs)
        self.model = model
        self.global_test_loader = global_test_loader
        self.num_classes = num_classes
        self.checkpoint_dir = checkpoint_dir
        self.results_dir = results_dir
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.global_results = []
        
        # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(checkpoint_dir, exist_ok=True)
        os.makedirs(results_dir, exist_ok=True)
        
        print(f"\nğŸ”§ GlobalEvaluationStrategy ì´ˆê¸°í™”")
        print(f"   - Device: {self.device}")
        print(f"   - Checkpoint Dir: {checkpoint_dir}")
        print(f"   - Results Dir: {results_dir}")
    
    def aggregate_fit(self, server_round, results, failures):
        """
        FedAvg ì§‘ê³„ í›„:
        1. ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        2. ê¸€ë¡œë²Œ í…ŒìŠ¤íŠ¸ì…‹ìœ¼ë¡œ í‰ê°€
        3. ê²°ê³¼ ì €ì¥
        """
        # FedAvg ê¸°ë³¸ ì§‘ê³„
        aggregated_parameters, aggregated_metrics = super().aggregate_fit(
            server_round, results, failures
        )
        
        if aggregated_parameters is not None:
            # 1. ì§‘ê³„ëœ íŒŒë¼ë¯¸í„°ë¥¼ ëª¨ë¸ì— ì ìš©
            parameters_ndarrays = fl.common.parameters_to_ndarrays(aggregated_parameters)
            set_parameters(self.model, parameters_ndarrays)
            
            # 2. ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            save_server_checkpoint(self.model, server_round, self.checkpoint_dir)
            
            # 3. ê¸€ë¡œë²Œ í…ŒìŠ¤íŠ¸ì…‹ìœ¼ë¡œ í‰ê°€
            eval_results = evaluate_global_model(
                self.model, 
                self.global_test_loader, 
                self.device, 
                server_round,
                self.num_classes
            )
            
            # 4. ê²°ê³¼ ì €ì¥
            self.global_results.append(eval_results)
            self.save_results()
            
            # 5. í´ë¼ì´ì–¸íŠ¸ ë¡œì»¬ ì„±ëŠ¥ë„ í•¨ê»˜ ì €ì¥
            self.save_client_metrics(server_round, results)
        
        return aggregated_parameters, aggregated_metrics
    
    def save_results(self):
        """ê¸€ë¡œë²Œ í‰ê°€ ê²°ê³¼ë¥¼ CSVì™€ JSONìœ¼ë¡œ ì €ì¥"""
        # CSVë¡œ ì €ì¥ (ì‹œê°í™” ìš©ì´)
        df_data = []
        for result in self.global_results:
            row = {
                'round': result['round'],
                'loss': result['loss'],
                'accuracy': result['accuracy'],
                'f1_macro': result['f1_macro'],
                'f1_weighted': result['f1_weighted']
            }
            # í´ë˜ìŠ¤ë³„ ì •í™•ë„ ì¶”ê°€
            for i, acc in enumerate(result['class_accuracy']):
                row[f'class_{i}_acc'] = acc * 100
            df_data.append(row)
        
        df = pd.DataFrame(df_data)
        csv_path = f"{self.results_dir}/global_evaluation_results.csv"
        df.to_csv(csv_path, index=False)
        
        # JSONìœ¼ë¡œ ì €ì¥ (ì „ì²´ ì •ë³´)
        json_path = f"{self.results_dir}/global_evaluation_results.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(self.global_results, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ ê¸€ë¡œë²Œ í‰ê°€ ê²°ê³¼ ì €ì¥:")
        print(f"   - CSV: {csv_path}")
        print(f"   - JSON: {json_path}")
    
    def save_client_metrics(self, server_round, results):
        """í´ë¼ì´ì–¸íŠ¸ ë¡œì»¬ í‰ê°€ ê²°ê³¼ ì €ì¥"""
        client_metrics = []
        
        for client_proxy, fit_res in results:
            metrics = fit_res.metrics
            client_metrics.append({
                'round': server_round,
                'client_id': client_proxy.cid,
                'num_examples': fit_res.num_examples,
                'metrics': metrics
            })
        
        # ë¼ìš´ë“œë³„ë¡œ ì €ì¥
        json_path = f"{self.results_dir}/client_metrics_round_{server_round}.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(client_metrics, f, indent=2, ensure_ascii=False)


def print_final_summary(results_dir="results"):
    """ìµœì¢… ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
    csv_path = f"{results_dir}/global_evaluation_results.csv"
    
    if os.path.exists(csv_path):
        df = pd.read_csv(csv_path)
        
        print(f"\n{'='*70}")
        print(f"ğŸ“Š ìµœì¢… ê²°ê³¼ ìš”ì•½")
        print(f"{'='*70}")
        print(f"\n  ì „ì²´ ë¼ìš´ë“œ í†µê³„:")
        print(f"     - ìµœì´ˆ ì •í™•ë„ (Round 1):  {df['accuracy'].iloc[0]:.2f}%")
        print(f"     - ìµœì¢… ì •í™•ë„ (Round {len(df)}): {df['accuracy'].iloc[-1]:.2f}%")
        print(f"     - ì •í™•ë„ í–¥ìƒ:            +{df['accuracy'].iloc[-1] - df['accuracy'].iloc[0]:.2f}%p")
        print(f"     - í‰ê·  ì •í™•ë„:            {df['accuracy'].mean():.2f}%")
        print(f"     - ìµœê³  ì •í™•ë„:            {df['accuracy'].max():.2f}% (Round {df['accuracy'].idxmax() + 1})")
        
        print(f"\n  F1 Score í†µê³„:")
        print(f"     - ìµœì¢… F1 (Macro):        {df['f1_macro'].iloc[-1]:.4f}")
        print(f"     - ìµœì¢… F1 (Weighted):     {df['f1_weighted'].iloc[-1]:.4f}")
        
        print(f"\n  í´ë˜ìŠ¤ë³„ ìµœì¢… ì •í™•ë„:")
        for i in range(9):
            col_name = f'class_{i}_acc'
            if col_name in df.columns:
                final_acc = df[col_name].iloc[-1]
                print(f"     Class {i}: {final_acc:.1f}%")
        
        print(f"{'='*70}\n")


def main():
    """ì„œë²„ ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='ê°œì„ ëœ ì—°í•©í•™ìŠµ ì„œë²„ (ê¸€ë¡œë²Œ í‰ê°€)')
    parser.add_argument('--num_rounds', type=int, default=20, help='ì—°í•©í•™ìŠµ ë¼ìš´ë“œ ìˆ˜')
    parser.add_argument('--num_clients', type=int, default=3, help='ì „ì²´ í´ë¼ì´ì–¸íŠ¸ ìˆ˜')
    parser.add_argument('--alpha', type=float, default=0.5, help='Dirichlet alpha (ì°¸ê³ ìš©)')
    parser.add_argument('--port', type=int, default=8080, help='ì„œë²„ í¬íŠ¸')
    parser.add_argument('--batch_size', type=int, default=128, help='ê¸€ë¡œë²Œ í…ŒìŠ¤íŠ¸ ë°°ì¹˜ í¬ê¸°')
    
    args = parser.parse_args()
    
    print("\n" + "="*70)
    print("ğŸŒ ê°œì„ ëœ ì—°í•©í•™ìŠµ ì„œë²„ - ê¸€ë¡œë²Œ í‰ê°€ ê¸°ëŠ¥")
    print("="*70)
    
    # ì‹¤í—˜ ì„¤ì •
    config = {
        'num_rounds': args.num_rounds,
        'num_clients': args.num_clients,
        'fraction_fit': 1.0,
        'min_fit_clients': args.num_clients,
        'min_available_clients': args.num_clients,
        'dataset': 'PathMNIST',
        'num_classes': 9,
        'model': 'SimpleCNN',
        'data_distribution': f'Non-IID (Dirichlet alpha={args.alpha})',
        'strategy': 'FedAvg + Global Evaluation'
    }
    
    print(f"\nâš™ï¸  ì„œë²„ ì„¤ì •:")
    print(f"  - ëª¨ë¸: {config['model']}")
    print(f"  - ë°ì´í„°ì…‹: {config['dataset']} ({config['num_classes']}ê°œ í´ë˜ìŠ¤)")
    print(f"  - ë°ì´í„° ë¶„í¬: {config['data_distribution']}")
    print(f"  - ì „ëµ: {config['strategy']}")
    print(f"  - ë¼ìš´ë“œ ìˆ˜: {config['num_rounds']}")
    print(f"  - í´ë¼ì´ì–¸íŠ¸ ìˆ˜: {config['num_clients']}")
    print(f"  - ì°¸ì—¬ ë¹„ìœ¨: {config['fraction_fit']*100:.0f}%")
    
    # ê¸€ë¡œë²Œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
    global_test_loader, num_classes = load_global_test_data(batch_size=args.batch_size)
    
    # ì´ˆê¸° ëª¨ë¸ ìƒì„±
    model = SimpleCNN(num_classes=num_classes)
    print(f"\nğŸ”§ SimpleCNN ëª¨ë¸ ìƒì„±")
    
    # íŒŒë¼ë¯¸í„° ìˆ˜ ì¶œë ¥
    total_params = sum(p.numel() for p in model.parameters())
    print(f"   ì´ íŒŒë¼ë¯¸í„° ìˆ˜: {total_params:,}")
    
    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹œë„
    start_round = load_server_checkpoint(model)
    
    initial_parameters = get_parameters(model)
    parameters = fl.common.ndarrays_to_parameters(initial_parameters)
    
    # ê¸€ë¡œë²Œ í‰ê°€ ì „ëµ ìƒì„±
    strategy = GlobalEvaluationStrategy(
        model=model,
        global_test_loader=global_test_loader,
        num_classes=num_classes,
        checkpoint_dir="checkpoints/server",
        results_dir="results",
        fraction_fit=config['fraction_fit'],
        fraction_evaluate=1.0,
        min_fit_clients=config['min_fit_clients'],
        min_evaluate_clients=config['num_clients'],
        min_available_clients=config['min_available_clients'],
        initial_parameters=parameters
    )
    
    print(f"\n{'='*70}")
    print(f"ğŸš€ ì„œë²„ ì‹œì‘")
    print(f"{'='*70}")
    print(f"  - ì£¼ì†Œ: 0.0.0.0:{args.port}")
    print(f"  - ëŒ€ê¸° ì¤‘: {config['num_clients']}ê°œ í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ëŒ€ê¸°")
    
    print(f"\nğŸ’¡ í´ë¼ì´ì–¸íŠ¸ ì‹¤í–‰ ëª…ë ¹ì–´ (í„°ë¯¸ë„ {config['num_clients']}ê°œ í•„ìš”):")
    print(f"{'='*70}")
    for i in range(config['num_clients']):
        print(f"  í„°ë¯¸ë„ {i+1}:")
        print(f"  python3 client.py --client_id {i} --total_clients {config['num_clients']} --alpha {args.alpha}")
        print()
    
    print(f"{'='*70}")
    print(f"â³ í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ëŒ€ê¸° ì¤‘...")
    print(f"{'='*70}\n")
    
    # ì„œë²„ ì‹œì‘
    try:
        fl.server.start_server(
            server_address=f"0.0.0.0:{args.port}",
            config=fl.server.ServerConfig(num_rounds=config['num_rounds']),
            strategy=strategy
        )
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ì„œë²„ ì¤‘ë‹¨ë¨")
    finally:
        # ìµœì¢… ê²°ê³¼ ìš”ì•½
        print_final_summary()
        
        print(f"\n{'='*70}")
        print(f"âœ… ì—°í•©í•™ìŠµ ì™„ë£Œ!")
        print(f"{'='*70}")
        print(f"\nğŸ“ ì €ì¥ëœ íŒŒì¼:")
        print(f"  - ì²´í¬í¬ì¸íŠ¸: checkpoints/server/")
        print(f"  - ê¸€ë¡œë²Œ í‰ê°€ ê²°ê³¼: results/global_evaluation_results.csv")
        print(f"  - ìƒì„¸ ê²°ê³¼: results/global_evaluation_results.json")
        print(f"  - í´ë¼ì´ì–¸íŠ¸ ë©”íŠ¸ë¦­: results/client_metrics_round_*.json")
        
        print(f"\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
        print(f"  1. results/global_evaluation_results.csvë¡œ ì‹œê°í™”")
        print(f"  2. ë¡œì»¬ í‰ê°€ vs ê¸€ë¡œë²Œ í‰ê°€ ë¹„êµ")
        print(f"  3. Baseline (ì¤‘ì•™ì§‘ì¤‘í˜•) ì‹¤í—˜ê³¼ ë¹„êµ")
        print(f"{'='*70}\n")


if __name__ == "__main__":
    main()