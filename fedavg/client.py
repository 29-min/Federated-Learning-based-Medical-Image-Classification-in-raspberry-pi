"""
7ì£¼ì°¨: ì—°í•©í•™ìŠµ í´ë¼ì´ì–¸íŠ¸ (Dirichlet ê¸°ë°˜ Non-IID)
PathMNIST (9ê°œ í´ë˜ìŠ¤)ë¥¼ Dirichlet Distributionìœ¼ë¡œ ë¶ˆê· ë“± ë¶„í• 
í˜„ì‹¤ì ì¸ ë³‘ì›ë³„ ë°ì´í„° ë¶„í¬ ì°¨ì´ë¥¼ ì‹œë®¬ë ˆì´ì…˜
"""

import flwr as fl
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Subset
import torchvision.transforms as transforms
import numpy as np
import argparse
import medmnist
from medmnist.info import INFO
from sklearn.metrics import f1_score

# GPU/CPU ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class SimpleCNN(nn.Module):
    def __init__(self, num_classes=9):
        super(SimpleCNN, self).__init__()
        
        # 3ì±„ë„ ì…ë ¥ (PathMNISTëŠ” RGB)
        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)
        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)
        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)
        
        self.pool = nn.MaxPool2d(2, 2)
        self.dropout = nn.Dropout(0.25)
        
        # PathMNIST 28x28 ì´ë¯¸ì§€
        self.fc1 = nn.Linear(64 * 3 * 3, 128)
        self.fc2 = nn.Linear(128, num_classes)
        
    def forward(self, x):
        x = self.pool(torch.relu(self.conv1(x)))
        x = self.pool(torch.relu(self.conv2(x)))
        x = self.pool(torch.relu(self.conv3(x)))
        
        x = x.view(x.size(0), -1)
        x = self.dropout(torch.relu(self.fc1(x)))
        x = self.fc2(x)
        
        return x

def dirichlet_split_noniid(labels, num_clients, num_classes, alpha, seed=42):
    """
    Dirichlet Distribution ê¸°ë°˜ Non-IID ë°ì´í„° ë¶„í• 
    
    Args:
        labels: ì „ì²´ ë°ì´í„°ì˜ ë¼ë²¨ ë°°ì—´ (numpy array)
        num_clients: í´ë¼ì´ì–¸íŠ¸ ìˆ˜
        num_classes: í´ë˜ìŠ¤ ìˆ˜
        alpha: Dirichlet ë¶„í¬ íŒŒë¼ë¯¸í„° (ì‘ì„ìˆ˜ë¡ ë” ë¶ˆê· ë“±)
               - alpha=0.1: ê·¹ë‹¨ì  Non-IID (ê° í´ë¼ì´ì–¸íŠ¸ê°€ 1-2ê°œ í´ë˜ìŠ¤ ì§‘ì¤‘)
               - alpha=0.5: ì¤‘ê°„ Non-IID (ê° í´ë¼ì´ì–¸íŠ¸ê°€ 3-4ê°œ í´ë˜ìŠ¤ ìœ„ì£¼)
               - alpha=10: ê±°ì˜ ê· ë“± (IIDì— ê°€ê¹Œì›€)
        seed: ëœë¤ ì‹œë“œ
    
    Returns:
        client_indices: ê° í´ë¼ì´ì–¸íŠ¸ê°€ ê°€ì ¸ê°ˆ ë°ì´í„° ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸
    """
    np.random.seed(seed)
    
    # 1ë‹¨ê³„: í´ë˜ìŠ¤ë³„ë¡œ ë°ì´í„° ì¸ë±ìŠ¤ ê·¸ë£¹í™”
    class_indices = [np.where(labels == i)[0] for i in range(num_classes)]
    
    # ê° í´ë˜ìŠ¤ë³„ ë°ì´í„° ê°œìˆ˜ í™•ì¸
    for i, indices in enumerate(class_indices):
        print(f"    í´ë˜ìŠ¤ {i}: {len(indices)}ê°œ ìƒ˜í”Œ")
    
    # 2ë‹¨ê³„: ê° í´ë¼ì´ì–¸íŠ¸ê°€ ë°›ì„ ì¸ë±ìŠ¤ ì €ì¥ì†Œ ì´ˆê¸°í™”
    client_indices = [[] for _ in range(num_clients)]
    
    # 3ë‹¨ê³„: ê° í´ë˜ìŠ¤ë¥¼ Dirichlet ë¶„í¬ë¡œ í´ë¼ì´ì–¸íŠ¸ë“¤ì—ê²Œ ë¶„ë°°
    for c_idx, c_indices in enumerate(class_indices):
        np.random.shuffle(c_indices)
        
        # Dirichlet ë¶„í¬ë¡œ ë¹„ìœ¨ ìƒì„±
        proportions = np.random.dirichlet(np.repeat(alpha, num_clients))
        
        # ë¹„ìœ¨ì„ ì‹¤ì œ ë°ì´í„° ê°œìˆ˜ë¡œ ë³€í™˜
        proportions = (np.cumsum(proportions) * len(c_indices)).astype(int)[:-1]
        
        # ì‹¤ì œë¡œ ë°ì´í„° ë¶„í• 
        split_indices = np.split(c_indices, proportions)
        
        # ê° í´ë¼ì´ì–¸íŠ¸ì—ê²Œ í• ë‹¹
        for client_id, indices in enumerate(split_indices):
            client_indices[client_id].extend(indices.tolist())
    
    # 4ë‹¨ê³„: ê° í´ë¼ì´ì–¸íŠ¸ ë‚´ì—ì„œ ë°ì´í„° ì„ê¸° (í•™ìŠµ íš¨ìœ¨ì„±)
    for client_id in range(num_clients):
        np.random.shuffle(client_indices[client_id])
    
    return client_indices

def analyze_class_distribution(dataset, indices):
    """í´ë˜ìŠ¤ ë¶„í¬ ë¶„ì„ ë° ì‹œê°í™”"""
    labels = []
    for idx in indices:
        _, label = dataset[idx]
        labels.append(label.item() if torch.is_tensor(label) else label)
    
    unique, counts = np.unique(labels, return_counts=True)
    distribution = dict(zip(unique, counts))
    
    return distribution

def load_data_dirichlet(client_id, total_clients, alpha=0.5, batch_size=32, seed=42):
    """
    Dirichlet ë¶„í¬ ê¸°ë°˜ Non-IID ë°ì´í„° ë¡œë“œ
    
    Args:
        client_id: í˜„ì¬ í´ë¼ì´ì–¸íŠ¸ ID
        total_clients: ì „ì²´ í´ë¼ì´ì–¸íŠ¸ ìˆ˜
        alpha: Dirichlet íŒŒë¼ë¯¸í„° (Non-IID ê°•ë„)
        batch_size: ë°°ì¹˜ í¬ê¸°
        seed: ëœë¤ ì‹œë“œ
    """
    
    # ë°ì´í„° ì „ì²˜ë¦¬
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5], std=[0.5])
    ])
    
    # PathMNIST ë¡œë“œ (9ê°œ í´ë˜ìŠ¤)
    data_flag = 'pathmnist'
    info = INFO[data_flag]
    num_classes = len(info['label'])
    
    print(f"\n{'='*60}")
    print(f"PathMNIST ì •ë³´:")
    print(f"  - í´ë˜ìŠ¤ ìˆ˜: {num_classes}")
    print(f"  - ì„¤ëª…: {info['description']}")
    print(f"{'='*60}\n")
    
    DataClass = getattr(medmnist, info['python_class'])
    
    # ì „ì²´ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ
    train_dataset = DataClass(split='train', transform=transform, download=True)
    test_dataset = DataClass(split='test', transform=transform, download=True)
    
    # ë¼ë²¨ ì¶”ì¶œ (numpy arrayë¡œ ë³€í™˜)
    train_labels = np.array([label.item() for _, label in train_dataset])
    test_labels = np.array([label.item() for _, label in test_dataset])
    
    print(f"ì „ì²´ ë°ì´í„°ì…‹ í¬ê¸°:")
    print(f"  - Train: {len(train_dataset)} samples")
    print(f"  - Test: {len(test_dataset)} samples")
    print(f"\nDirichlet ë¶„í•  ì‹œì‘ (alpha={alpha})...")
    print(f"  â€» alphaê°€ ì‘ì„ìˆ˜ë¡ í´ë˜ìŠ¤ ë¶„í¬ê°€ ë¶ˆê· ë“±í•´ì§‘ë‹ˆë‹¤")
    print(f"\nTrain ë°ì´í„° í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ìˆ˜:")
    
    # Dirichlet ë¶„í•  ìˆ˜í–‰
    train_client_indices = dirichlet_split_noniid(
        train_labels, 
        total_clients, 
        num_classes, 
        alpha, 
        seed
    )
    
    print(f"\nTest ë°ì´í„° í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ìˆ˜:")
    test_client_indices = dirichlet_split_noniid(
        test_labels, 
        total_clients, 
        num_classes, 
        alpha, 
        seed + 1  # ë‹¤ë¥¸ ì‹œë“œë¡œ ë…ë¦½ì  ë¶„í• 
    )
    
    # í˜„ì¬ í´ë¼ì´ì–¸íŠ¸ì˜ ì¸ë±ìŠ¤ ê°€ì ¸ì˜¤ê¸°
    client_train_indices = train_client_indices[client_id]
    client_test_indices = test_client_indices[client_id]
    
    # í´ë˜ìŠ¤ ë¶„í¬ ë¶„ì„
    train_distribution = analyze_class_distribution(train_dataset, client_train_indices)
    test_distribution = analyze_class_distribution(test_dataset, client_test_indices)
    
    # Subset ìƒì„±
    train_subset = Subset(train_dataset, client_train_indices)
    test_subset = Subset(test_dataset, client_test_indices)
    
    # DataLoader
    train_loader = DataLoader(
        train_subset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=0
    )
    
    test_loader = DataLoader(
        test_subset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=0
    )
    
    # ìƒì„¸ ì •ë³´ ì¶œë ¥
    print(f"\n{'='*60}")
    print(f"í´ë¼ì´ì–¸íŠ¸ {client_id} ë°ì´í„° í• ë‹¹ ì™„ë£Œ")
    print(f"{'='*60}")
    print(f"\ní• ë‹¹ëœ ë°ì´í„°:")
    print(f"  - Train: {len(train_subset)} samples ({len(train_subset)/len(train_dataset)*100:.1f}%)")
    print(f"  - Test: {len(test_subset)} samples ({len(test_subset)/len(test_dataset)*100:.1f}%)")
    
    print(f"\ní´ë˜ìŠ¤ ë¶„í¬ (Train):")
    print(f"  {'í´ë˜ìŠ¤':<8} {'ìƒ˜í”Œ ìˆ˜':<10} {'ë¹„ìœ¨':<10} {'ì‹œê°í™”'}")
    print(f"  {'-'*50}")
    for label in range(num_classes):
        count = train_distribution.get(label, 0)
        percentage = count / len(train_subset) * 100
        bar = 'â–ˆ' * int(percentage / 2)  # 50%ë‹¹ 25ì¹¸
        print(f"  {label:<8} {count:<10} {percentage:>5.1f}%     {bar}")
    
    print(f"\ní´ë˜ìŠ¤ ë¶„í¬ (Test):")
    print(f"  {'í´ë˜ìŠ¤':<8} {'ìƒ˜í”Œ ìˆ˜':<10} {'ë¹„ìœ¨':<10} {'ì‹œê°í™”'}")
    print(f"  {'-'*50}")
    for label in range(num_classes):
        count = test_distribution.get(label, 0)
        percentage = count / len(test_subset) * 100
        bar = 'â–ˆ' * int(percentage / 2)
        print(f"  {label:<8} {count:<10} {percentage:>5.1f}%     {bar}")
    
    print(f"{'='*60}\n")
    
    return train_loader, test_loader, num_classes

def calculate_class_weights(train_loader, num_classes=9):
    """í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°"""
    class_counts = torch.zeros(num_classes)
    
    for _, labels in train_loader:
        labels = labels.squeeze()
        for label in labels:
            class_counts[label.item()] += 1
    
    # ì—­ìˆ˜ë¡œ ê°€ì¤‘ì¹˜ ê³„ì‚° (í¬ê·€ í´ë˜ìŠ¤ì— ë†’ì€ ê°€ì¤‘ì¹˜)
    total = class_counts.sum()
    class_weights = total / (num_classes * class_counts)
    
    # 0ê°œì¸ í´ë˜ìŠ¤ëŠ” ê°€ì¤‘ì¹˜ 0 (í•™ìŠµ ì•ˆ í•¨)
    class_weights[class_counts == 0] = 0.0
    
    return class_weights

def train(model, train_loader, epochs=5, lr=0.01):
    """ë¡œì»¬ í•™ìŠµ (í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©, F1 score ê³„ì‚°)"""
    # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°
    class_weights = calculate_class_weights(train_loader).to(device)
    
    criterion = nn.CrossEntropyLoss(weight=class_weights)
    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)
    
    model.train()
    
    for epoch in range(epochs):
        running_loss = 0.0
        correct = 0
        total = 0
        
        # F1 score ê³„ì‚°ì„ ìœ„í•œ ì˜ˆì¸¡ ë° ì‹¤ì œ ë¼ë²¨ ì €ì¥
        all_predictions = []
        all_targets = []
        
        for data, target in train_loader:
            data, target = data.to(device), target.to(device).squeeze().long()
            
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()
            
            running_loss += loss.item()
            _, predicted = torch.max(output.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()
            
            # F1 score ê³„ì‚°ì„ ìœ„í•´ ì €ì¥
            all_predictions.extend(predicted.cpu().numpy())
            all_targets.extend(target.cpu().numpy())
        
        epoch_loss = running_loss / len(train_loader)
        epoch_acc = 100. * correct / total
        
        # F1 score ê³„ì‚° (macro average)
        epoch_f1 = f1_score(all_targets, all_predictions, average='macro', zero_division=0)
        
        print(f"  Epoch {epoch+1}/{epochs}: Loss={epoch_loss:.4f}, Acc={epoch_acc:.2f}%, F1={epoch_f1:.4f}")
    
    avg_loss = running_loss / len(train_loader)
    avg_acc = 100. * correct / total
    avg_f1 = f1_score(all_targets, all_predictions, average='macro', zero_division=0)
    
    return avg_loss, avg_acc, avg_f1

def test(model, test_loader):
    """ë¡œì»¬ í‰ê°€ (F1 score í¬í•¨)"""
    criterion = nn.CrossEntropyLoss()
    model.eval()
    
    test_loss = 0
    correct = 0
    total = 0
    
    # F1 score ê³„ì‚°ì„ ìœ„í•œ ì˜ˆì¸¡ ë° ì‹¤ì œ ë¼ë²¨ ì €ì¥
    all_predictions = []
    all_targets = []
    
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device).squeeze().long()
            output = model(data)
            test_loss += criterion(output, target).item()
            
            _, predicted = torch.max(output.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()
            
            # F1 score ê³„ì‚°ì„ ìœ„í•´ ì €ì¥
            all_predictions.extend(predicted.cpu().numpy())
            all_targets.extend(target.cpu().numpy())
    
    test_loss /= len(test_loader)
    test_acc = 100. * correct / total
    
    # F1 score ê³„ì‚° (macro average)
    test_f1 = f1_score(all_targets, all_predictions, average='macro', zero_division=0)
    
    return test_loss, test_acc, test_f1

class FlowerClient(fl.client.NumPyClient):
    """Flower í´ë¼ì´ì–¸íŠ¸ (ì²´í¬í¬ì¸íŠ¸ ê¸°ëŠ¥ ì¶”ê°€)"""
    
    def __init__(self, client_id, model, train_loader, test_loader, local_epochs, lr):
        self.client_id = client_id
        self.model = model
        self.train_loader = train_loader
        self.test_loader = test_loader
        self.local_epochs = local_epochs
        self.lr = lr
        self.checkpoint_dir = f"checkpoints/client_{client_id}"
        
        # ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ ìƒì„±
        import os
        os.makedirs(self.checkpoint_dir, exist_ok=True)
        
        # ë§ˆì§€ë§‰ ë¼ìš´ë“œ ë²ˆí˜¸ ì¶”ì 
        self.current_round = 0
        
        # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹œë„
        self.load_checkpoint()
    
    def save_checkpoint(self):
        """í˜„ì¬ ëª¨ë¸ ìƒíƒœ ì €ì¥"""
        checkpoint_path = f"{self.checkpoint_dir}/round_{self.current_round}.pt"
        torch.save({
            'round': self.current_round,
            'model_state_dict': self.model.state_dict(),
        }, checkpoint_path)
        print(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {checkpoint_path}")
    
    def load_checkpoint(self):
        """ê°€ì¥ ìµœê·¼ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        import os
        import glob
        
        # ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ ì°¾ê¸°
        checkpoints = glob.glob(f"{self.checkpoint_dir}/round_*.pt")
        
        if checkpoints:
            # ê°€ì¥ ìµœê·¼ ë¼ìš´ë“œ ì°¾ê¸°
            latest_checkpoint = max(checkpoints, key=lambda x: int(x.split('_')[-1].split('.')[0]))
            
            checkpoint = torch.load(latest_checkpoint)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.current_round = checkpoint['round']
            
            print(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {latest_checkpoint}")
            print(f"   ë¼ìš´ë“œ {self.current_round}ë¶€í„° ì¬ê°œí•©ë‹ˆë‹¤.")
        else:
            print(f"ğŸ†• ìƒˆë¡œìš´ í•™ìŠµ ì‹œì‘")
    
    def get_parameters(self, config):
        """ëª¨ë¸ íŒŒë¼ë¯¸í„° ë°˜í™˜"""
        return [val.cpu().numpy() for _, val in self.model.state_dict().items()]
    
    def set_parameters(self, parameters):
        """ì„œë²„ íŒŒë¼ë¯¸í„° ì„¤ì •"""
        params_dict = zip(self.model.state_dict().keys(), parameters)
        state_dict = {k: torch.tensor(v) for k, v in params_dict}
        self.model.load_state_dict(state_dict, strict=True)
    
    def fit(self, parameters, config):
        """ë¡œì»¬ í•™ìŠµ"""
        self.current_round += 1
        
        print(f"\n{'='*60}")
        print(f"í´ë¼ì´ì–¸íŠ¸ {self.client_id} - ë¼ìš´ë“œ {self.current_round} í•™ìŠµ ì‹œì‘")
        print(f"{'='*60}")
        
        self.set_parameters(parameters)
        
        train_loss, train_acc, train_f1 = train(
            self.model,
            self.train_loader,
            epochs=self.local_epochs,
            lr=self.lr
        )
        
        # í•™ìŠµ ì™„ë£Œ í›„ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        self.save_checkpoint()
        
        print(f"í•™ìŠµ ì™„ë£Œ: Loss={train_loss:.4f}, Acc={train_acc:.2f}%, F1={train_f1:.4f}")
        print(f"{'='*60}\n")
        
        return self.get_parameters(config={}), len(self.train_loader.dataset), {"f1_score": train_f1}
    
    def evaluate(self, parameters, config):
        """ë¡œì»¬ í‰ê°€"""
        print(f"\ní´ë¼ì´ì–¸íŠ¸ {self.client_id} - ë¼ìš´ë“œ {self.current_round} í‰ê°€ ì‹œì‘")
        
        self.set_parameters(parameters)
        
        test_loss, test_acc, test_f1 = test(self.model, self.test_loader)
        
        print(f"í‰ê°€ ì™„ë£Œ: Loss={test_loss:.4f}, Acc={test_acc:.2f}%, F1={test_f1:.4f}\n")
        
        return test_loss, len(self.test_loader.dataset), {"accuracy": test_acc, "f1_score": test_f1}

def main():
    """í´ë¼ì´ì–¸íŠ¸ ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='ì—°í•©í•™ìŠµ í´ë¼ì´ì–¸íŠ¸ (Dirichlet Non-IID)')
    parser.add_argument('--client_id', type=int, required=True, help='í´ë¼ì´ì–¸íŠ¸ ID')
    parser.add_argument('--total_clients', type=int, default=3, help='ì „ì²´ í´ë¼ì´ì–¸íŠ¸ ìˆ˜')
    parser.add_argument('--alpha', type=float, default=0.5, help='Dirichlet alpha (ì‘ì„ìˆ˜ë¡ Non-IID)')
    parser.add_argument('--server_address', type=str, default='192.168.45.100:8080', help='ì„œë²„ ì£¼ì†Œ')
    parser.add_argument('--local_epochs', type=int, default=5, help='ë¡œì»¬ ì—í¬í¬')
    parser.add_argument('--batch_size', type=int, default=32, help='ë°°ì¹˜ í¬ê¸°')
    parser.add_argument('--lr', type=float, default=0.01, help='í•™ìŠµë¥ ')
    
    args = parser.parse_args()
    
    print("\n" + "="*60)
    print(f"ì—°í•©í•™ìŠµ í´ë¼ì´ì–¸íŠ¸ {args.client_id} (Dirichlet Non-IID)")
    print("="*60)
    print(f"ì„œë²„: {args.server_address}")
    print(f"ì „ì²´ í´ë¼ì´ì–¸íŠ¸: {args.total_clients}")
    print(f"ëª¨ë¸: SimpleCNN")
    print(f"Alpha (Non-IID ê°•ë„): {args.alpha}")
    print(f"  â€» alpha=0.1: ê·¹ë‹¨ì  Non-IID")
    print(f"  â€» alpha=0.5: ì¤‘ê°„ Non-IID")
    print(f"  â€» alpha=10: ê±°ì˜ IID")
    print(f"ë¡œì»¬ ì—í¬í¬: {args.local_epochs}")
    print(f"ë°°ì¹˜ í¬ê¸°: {args.batch_size}")
    print(f"í•™ìŠµë¥ : {args.lr}")
    print("="*60)
    
    # ì‹œë“œ ì„¤ì • (ì¬í˜„ì„±)
    torch.manual_seed(42)
    np.random.seed(42)
    
    # ë°ì´í„° ë¡œë“œ (Dirichlet ë¶„í• )
    train_loader, test_loader, num_classes = load_data_dirichlet(
        args.client_id,
        args.total_clients,
        args.alpha,
        args.batch_size
    )
    
    # ëª¨ë¸ ìƒì„±
    model = SimpleCNN(num_classes=num_classes).to(device)
    print(f"\nğŸ”§ SimpleCNN ëª¨ë¸ ìƒì„± ì™„ë£Œ")
    
    # íŒŒë¼ë¯¸í„° ìˆ˜ ì¶œë ¥
    total_params = sum(p.numel() for p in model.parameters())
    print(f"   ì´ íŒŒë¼ë¯¸í„° ìˆ˜: {total_params:,}")
    
    # Flower í´ë¼ì´ì–¸íŠ¸ ìƒì„±
    client = FlowerClient(
        args.client_id,
        model,
        train_loader,
        test_loader,
        args.local_epochs,
        args.lr
    )
    
    print(f"\nì„œë²„ {args.server_address}ì— ì—°ê²° ì¤‘...")
    
    # ì„œë²„ ì—°ê²°
    fl.client.start_client(
        server_address=args.server_address,
        client=client
    )
    
    print(f"\ní´ë¼ì´ì–¸íŠ¸ {args.client_id} ì¢…ë£Œ")

if __name__ == "__main__":
    main()